{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b1b5c-5736-4b90-869d-305874a28c75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, log_loss, confusion_matrix\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ==========================================\n",
    "# 1. 基础配置：定义列索引\n",
    "# ==========================================\n",
    "cat_indices = [1,3,4,5,6,7,8,9]        \n",
    "cont_indices = [2]                \n",
    "item_indices = list(range(10, 80))      \n",
    "\n",
    "# 目标变量索引\n",
    "target_index = 91 \n",
    "\n",
    "# 常用集合\n",
    "demographics = cat_indices + cont_indices\n",
    "all_items = item_indices                 \n",
    "\n",
    "# ==========================================\n",
    "# 2. 模型特征配置\n",
    "# ==========================================\n",
    "model_feature_config = {\n",
    "    'LASSO_LR': [2,71,80,72,73,76,78,79,24,29,30,35,37,\n",
    "                12,43,14,16,18,62,47,65,67,68],           \n",
    "    'SVM': [16,43,37,68,2,76,75,80,24,7,62,21,72,78,57,\n",
    "                20,67,18],                                \n",
    "    'XGBoost': [62,80,76,68,67,16,73,65,60,72,75,78,43,\n",
    "               77,47,71,79,70,37,23,21,64,22,50,12,74,\n",
    "               61,18,63,14,19,28],                        \n",
    "    'LightGBM': [76,80,62,16,43,2,72,68],                 \n",
    "    'Random Forest': [80,76,68,79,78,77,16,67,72,43,60,\n",
    "                     62,75,70,71,73,50,47,64,12,2,65]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414b58e-9290-4052-a6e4-8ff4305c4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. 数据读取与智能清洗\n",
    "# ==========================================\n",
    "file_path = \"患者筛选数据0110.xlsx\"\n",
    "df = pd.read_excel(file_path, header=2)\n",
    "\n",
    "if target_index < len(df.columns):\n",
    "    df = df.dropna(subset=[df.columns[target_index]])\n",
    "else:\n",
    "    raise ValueError(f\"目标列索引 {target_index} 超出了数据列数范围 (总列数: {len(df.columns)})\")\n",
    "\n",
    "cat_names = [df.columns[i] for i in cat_indices if i < len(df.columns)]\n",
    "cont_names = [df.columns[i] for i in cont_indices if i < len(df.columns)]\n",
    "item_names = [df.columns[i] for i in item_indices if i < len(df.columns)]\n",
    "all_cont_names = cont_names + item_names \n",
    "\n",
    "\n",
    "for col in all_cont_names:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac74ce-87d2-4ce9-9323-eb6a2b237dde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. 准备建模数据\n",
    "# ==========================================\n",
    "all_needed_indices = set()\n",
    "for indices in model_feature_config.values():\n",
    "    all_needed_indices.update(indices)\n",
    "all_needed_indices.add(target_index)\n",
    "\n",
    "# 将索引转为列名\n",
    "all_needed_cols = [df.columns[i] for i in all_needed_indices if i < len(df.columns)]\n",
    "\n",
    "# 剔除缺失值 \n",
    "df_clean = df.dropna(subset=[df.columns[target_index]]).copy()\n",
    "print(f\"原始样本量: {len(df)}\")\n",
    "print(f\"清洗后有效样本量 (仅剔除无标签样本): {len(df_clean)}\")\n",
    "\n",
    "# 4.1 如果样本量为0，打印错误信息\n",
    "if len(df_clean) == 0:\n",
    "    print(\"\\n【严重错误】有效样本量为0！请检查以下缺失值情况：\")\n",
    "    print(\"(显示缺失值最多的前10个列)\")\n",
    "    na_counts = df[all_needed_cols].isna().sum().sort_values(ascending=False)\n",
    "    print(na_counts.head(10))\n",
    "    raise ValueError(\"数据清洗后没有剩余样本，请检查上述列的数据质量或索引配置。\")\n",
    "\n",
    "# 1. 划分索引 (Train/Test)\n",
    "train_idx, test_idx = train_test_split(df_clean.index, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. 计算分类阈值\n",
    "y_score_train = df_clean.loc[train_idx, df.columns[target_index]].astype(float)\n",
    "y_score_test = df_clean.loc[test_idx, df.columns[target_index]].astype(float)\n",
    "\n",
    "# 3. 在训练集上计算阈值\n",
    "q1 = y_score_train.quantile(1/3)\n",
    "q2 = y_score_train.quantile(2/3)\n",
    "bins = [-np.inf, q1, q2, np.inf]\n",
    "\n",
    "print(f\"Label cut thresholds (calculated on Train only): Q1={q1:.2f}, Q2={q2:.2f}\")\n",
    "\n",
    "# 4. 应用阈值生成标签 (0:Low, 1:Medium, 2:High)\n",
    "y_train = pd.cut(y_score_train, bins=bins, labels=[0, 1, 2], include_lowest=True).astype(int)\n",
    "y_test = pd.cut(y_score_test, bins=bins, labels=[0, 1, 2], include_lowest=True).astype(int)\n",
    "\n",
    "print(\"Train class counts:\", y_train.value_counts().sort_index().values)\n",
    "print(\"Test class counts: \", y_test.value_counts().sort_index().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d417e3-d2f6-4093-be29-392dacc0f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4703fa-6eda-4668-a4cf-02c9128b57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. 网格搜索与模型训练\n",
    "# ==========================================\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score\n",
    "\n",
    "models = {\n",
    "    'LASSO_LR': LogisticRegression(solver='saga', penalty='elasticnet', max_iter=10000, random_state=42),\n",
    "    \n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    \n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    \n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    \n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "}\n",
    "# 5.1 定义超参数网格\n",
    "param_grids = {\n",
    "    'LASSO_LR': {\n",
    "        'classifier__l1_ratio': [0.1, 0.5, 0.9], \n",
    "        'classifier__C': [0.01, 0.1, 1, 4, 10] \n",
    "    },\n",
    "    \n",
    "    'Random Forest': {\n",
    "        'classifier__n_estimators': [100, 500, 1000], \n",
    "        'classifier__max_features': ['sqrt', None], \n",
    "        'classifier__max_depth': [None, 10, 20],   \n",
    "        'classifier__min_samples_split': [2, 5]\n",
    "    },\n",
    "    \n",
    "    'XGBoost': {\n",
    "        'classifier__max_depth': [3, 6, 11], \n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.3], \n",
    "        'classifier__n_estimators': [200,300,500, 1000],\n",
    "        'classifier__reg_alpha': [0, 0.1, 1],     \n",
    "        'classifier__reg_lambda': [1, 5]         \n",
    "    },\n",
    "    \n",
    "    'SVM': {\n",
    "        'classifier__C': [0.1, 1, 4, 10], \n",
    "        'classifier__gamma': ['scale', 0.135],    \n",
    "        'classifier__kernel': ['rbf']             \n",
    "    },\n",
    "    \n",
    "    'LightGBM': {\n",
    "        'classifier__learning_rate': [0.01, 0.05],\n",
    "        'classifier__n_estimators': [500, 1000],\n",
    "        'classifier__num_leaves': [31, 50],\n",
    "        'classifier__reg_alpha': [0.1, 1]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 5.2 定义优化目标：Macro-AUC (One-vs-Rest)\n",
    "scoring_metric = 'roc_auc_ovr'\n",
    "\n",
    "# 存储最佳模型\n",
    "best_models = {} \n",
    "\n",
    "print(f\"\\n开始 5折交叉验证 Grid Search (优化目标: {scoring_metric})...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, model_base in models.items():\n",
    "    print(f\"正在优化模型: {name} ...\")\n",
    "    \n",
    "    # --- A. 准备数据特征 ---\n",
    "    feature_indices = model_feature_config[name]\n",
    "    valid_indices = [i for i in feature_indices if i < len(df.columns)]\n",
    "    feature_names = [df.columns[i] for i in valid_indices]\n",
    "    \n",
    "    current_cat = [f for f in feature_names if f in cat_names]\n",
    "    current_cont = [f for f in feature_names if f not in cat_names]\n",
    "    \n",
    "    X_train_curr = df_clean.loc[train_idx, feature_names]\n",
    "    \n",
    "    # --- B. 构建 Pipeline ---\n",
    "    transformers = []\n",
    "    if current_cont:\n",
    "        transformers.append(('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')), \n",
    "            ('scaler', StandardScaler())\n",
    "        ]), current_cont))\n",
    "    if current_cat:\n",
    "        transformers.append(('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')), \n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), current_cat))\n",
    "        \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', ColumnTransformer(transformers=transformers)),\n",
    "        ('classifier', model_base)\n",
    "    ])\n",
    "    \n",
    "    # --- C. 执行网格搜索 ---\n",
    "    current_grid = param_grids.get(name, {})\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=current_grid,\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring=scoring_metric,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # 训练\n",
    "    grid.fit(X_train_curr, y_train)\n",
    "    \n",
    "    # --- D. 保存与记录 ---\n",
    "    best_models[name] = grid.best_estimator_\n",
    "    \n",
    "    print(f\"  > 最佳 AUC (CV平均): {grid.best_score_:.4f}\")\n",
    "    print(f\"  > 最佳参数: {grid.best_params_}\")\n",
    "    \n",
    "    # 简单测试\n",
    "    X_test_curr = df_clean.loc[test_idx, feature_names]\n",
    "    y_test_pred = grid.best_estimator_.predict(X_test_curr)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"  > 独立测试集 Accuracy: {test_acc:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n网格搜索完成，最佳模型已保存至 best_models 字典中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b345d8-4b24-4f3b-8a21-fcd9b4168195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SHAP 可解释性分析\n",
    "# ==========================================\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- A. 绘图风格设置 ---\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# 定义所有类别\n",
    "CLASS_MAPPING = {\n",
    "    0: \"Low Trust\", \n",
    "    1: \"Medium Trust\", \n",
    "    2: \"High Trust\"\n",
    "}\n",
    "\n",
    "print(f\"开始执行全量 SHAP 分析...\")\n",
    "print(f\"计划生成: {len(best_models)} 个模型 x 3 个类别 x 2 种图表 = {len(best_models)*6} 张图片\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# --- B. 辅助函数 ---\n",
    "def get_transformed_feature_names(preprocessor):\n",
    "    output_features = []\n",
    "    for name, pipe, features in preprocessor.transformers_:\n",
    "        if name == 'remainder': continue\n",
    "        if hasattr(pipe, 'get_feature_names_out'):\n",
    "            output_features.extend(pipe.get_feature_names_out(features))\n",
    "        else:\n",
    "            output_features.extend(features)\n",
    "    return output_features\n",
    "\n",
    "# --- C. 主循环：遍历每一个模型 ---\n",
    "for name, model_pipeline in best_models.items():\n",
    "    print(f\"\\n>> 正在处理模型: {name} ...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. 拆解 Pipeline & 准备数据\n",
    "        preprocessor = model_pipeline.named_steps['preprocessor']\n",
    "        classifier = model_pipeline.named_steps['classifier']\n",
    "        feature_indices = model_feature_config[name]\n",
    "        feature_names_raw = [df.columns[i] for i in feature_indices]\n",
    "        \n",
    "        # 提取数据\n",
    "        X_test_raw = df_clean.loc[test_idx, feature_names_raw]\n",
    "        X_train_raw = df_clean.loc[train_idx, feature_names_raw]\n",
    "        \n",
    "        # 转换数据\n",
    "        X_test_trans = preprocessor.transform(X_test_raw)\n",
    "        X_train_trans = preprocessor.transform(X_train_raw)\n",
    "        feature_names_final = get_transformed_feature_names(preprocessor)\n",
    "        \n",
    "        # 2.SHAP Values\n",
    "        explainer = None\n",
    "        shap_values_all = None\n",
    "        \n",
    "        # (a) 树模型\n",
    "        if \"XGB\" in name or \"Random\" in name or \"LightGBM\" in name or \"Tree\" in name:\n",
    "            explainer = shap.TreeExplainer(classifier)\n",
    "            shap_values_all = explainer.shap_values(X_test_trans) # 通常返回 list of arrays\n",
    "            \n",
    "        # (b) 线性模型\n",
    "        elif \"LR\" in name or \"Logistic\" in name or \"Lasso\" in name:\n",
    "            masker = shap.maskers.Independent(data=X_train_trans)\n",
    "            explainer = shap.LinearExplainer(classifier, masker=masker)\n",
    "            shap_values_all = explainer.shap_values(X_test_trans)\n",
    "            \n",
    "        # (c) 核模型 (SVM等)\n",
    "        else:\n",
    "            background_summary = shap.kmeans(X_train_trans, 30) \n",
    "            def predict_proba_wrapper(x):\n",
    "                return classifier.predict_proba(x) \n",
    "            \n",
    "            explainer = shap.KernelExplainer(predict_proba_wrapper, background_summary)\n",
    "            X_test_sample = X_test_trans[:50] \n",
    "            shap_values_all = explainer.shap_values(X_test_sample)\n",
    "            X_test_trans = X_test_sample \n",
    "\n",
    "        # 3. 内层循环：遍历所有类别 (Low/Medium/High)\n",
    "        if not isinstance(shap_values_all, list):\n",
    "            if len(shap_values_all.shape) == 3: \n",
    "                shap_values_all = [shap_values_all[:,:,i] for i in range(shap_values_all.shape[2])]\n",
    "            else:\n",
    "                shap_values_all = [1-shap_values_all, shap_values_all] \n",
    "\n",
    "        for class_idx, class_label in CLASS_MAPPING.items():\n",
    "            if class_idx >= len(shap_values_all): continue\n",
    "            \n",
    "            shap_val_target = shap_values_all[class_idx]\n",
    "            \n",
    "            # --- Beeswarm ---\n",
    "            plt.figure(figsize=(10, 8), dpi=300)\n",
    "            shap.summary_plot(shap_val_target, X_test_trans, feature_names=feature_names_final, show=False)\n",
    "            plt.title(f\"SHAP Summary: {name} - {class_label}\\n(Derivation Cohort)\", fontsize=16, pad=20)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            fname_bee = f\"SHAP_Beeswarm_{name.replace(' ', '')}_{class_label.replace(' ', '')}\"\n",
    "            plt.savefig(f\"{fname_bee}.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.savefig(f\"{fname_bee}.pdf\", bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # --- Importance---\n",
    "            plt.figure(figsize=(10, 8), dpi=300)\n",
    "            shap.summary_plot(shap_val_target, X_test_trans, feature_names=feature_names_final, plot_type=\"bar\", show=False)\n",
    "            plt.title(f\"Feature Importance: {name} - {class_label}\\n(Derivation Cohort)\", fontsize=16, pad=20)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            fname_bar = f\"SHAP_Bar_{name.replace(' ', '')}_{class_label.replace(' ', '')}\"\n",
    "            plt.savefig(f\"{fname_bar}.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.savefig(f\"{fname_bar}.pdf\", bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"   -> 已生成: {class_label} 的图表\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!! 模型 {name} SHAP 生成出错: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n 所有 SHAP 图片生成完毕。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
